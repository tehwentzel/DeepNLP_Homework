{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from multiprocessing.pool import Pool\n",
    "import re\n",
    "positive_file = 'data/rt-polarity.pos'\n",
    "negative_file = 'data/rt-polarity.neg'\n",
    "data_root = 'data/stanfordSentimentTreebank/'\n",
    "glove_pattern = 'data/glove.6B.<size>d.txt'\n",
    "glove_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dict = None\n",
    "def get_glove(size = 50):\n",
    "    global glove_dict\n",
    "    if glove_dict is None or len(list(glove_dict.values())[0]) != size:\n",
    "        file = re.sub('<size>', str(size), glove_pattern)\n",
    "        glove = pd.read_csv(file, sep = \" \", header = None, index_col = 0, quoting = 3)\n",
    "        glove_dict = {key: val.values for key, val in glove.T.items()}\n",
    "    return glove_dict\n",
    "\n",
    "def preprocess_sentence(line):\n",
    "    line = re.sub(r'[^\\x00-\\x7F]+', '', line.strip())\n",
    "    return line.strip().lower()\n",
    "\n",
    "def preprocess_for_labels(line):\n",
    "    line = preprocess_sentence(line)\n",
    "    line = re.sub('\\W+', '', line.strip())\n",
    "    return line\n",
    "    \n",
    "def get_labelset(file):\n",
    "    with open(file, \"rt\", encoding=\"utf-8\") as f:\n",
    "        sentences = set([preprocess_for_labels(line) for line in f.readlines()])\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_labelset = get_labelset(positive_file)\n",
    "negative_labelset = get_labelset(negative_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split_dict = {}\n",
    "data_split_map = {'1': 'train', '2': 'test', '3':'val'}\n",
    "with open(data_root + 'datasetSplit.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        [index, set_code] = line.strip().split(',')\n",
    "        try:\n",
    "            index = int(index)\n",
    "        except:\n",
    "            continue\n",
    "        if set_code not in data_split_map:\n",
    "            print(index, set_code)\n",
    "        data_split_dict[index] = data_split_map.get(set_code, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sentiment(sentence):\n",
    "    s = preprocess_for_labels(sentence)\n",
    "    for y, labelset in enumerate([negative_labelset, positive_labelset]):\n",
    "        for entry in labelset:\n",
    "            if s in labelset:\n",
    "                return y\n",
    "    return -1\n",
    "\n",
    "sentences = {}\n",
    "word2ind = {}\n",
    "ttws = tf.keras.preprocessing.text.text_to_word_sequence\n",
    "n_skipped = 0\n",
    "idx = 0\n",
    "with open(data_root + 'datasetSentences.txt') as data:\n",
    "    lines = data.readlines()\n",
    "    for line in lines:\n",
    "        index = re.search('^\\d+', line)\n",
    "        if index is None:\n",
    "            continue\n",
    "        index = int(index.group())\n",
    "        if index not in data_split_dict:\n",
    "            print(index, line)\n",
    "        entry = {'split_set': data_split_dict.get(index, 'val')}\n",
    "        line = preprocess_sentence(line)\n",
    "        line = re.sub(r'^\\d+\\s+', '', line)\n",
    "        sentiment = check_sentiment(line)\n",
    "        if sentiment >= 0:\n",
    "            entry['y'] = sentiment\n",
    "        else:\n",
    "            n_skipped += 1\n",
    "            continue\n",
    "        sentences[line] = entry\n",
    "        tokens = ttws(line)\n",
    "        for token in tokens:\n",
    "            if token not in word2ind:\n",
    "                word2ind[token] = idx\n",
    "                idx = idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2775  skipped\n",
      "9077  kept\n"
     ]
    }
   ],
   "source": [
    "print(n_skipped, ' skipped')\n",
    "print(len(sentences), ' kept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splits = {title: {'x':[], 'y':[]} for title in ['train', 'test', 'val']}\n",
    "for line, entry in sentences.items():\n",
    "    which = entry['split_set']\n",
    "    tokenized = [int(word2ind[token]) for token in ttws(line)]\n",
    "    (data_splits[which]['x']).append(tokenized)\n",
    "    (data_splits[which]['y']).append(int(entry['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset size: 6530\n",
      "test dataset size: 1702\n",
      "val dataset size: 845\n"
     ]
    }
   ],
   "source": [
    "for title in ['train', 'test', 'val']:\n",
    "    print(title + ' dataset size:', len(data_splits[title]['x']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(label, sequence_len):\n",
    "    vals = data_splits[label]\n",
    "    x = [np.array(vx) for vx in vals['x']]\n",
    "    x = tf.keras.preprocessing.sequence.pad_sequences(x,\n",
    "                                                      padding = 'post',\n",
    "                                                      maxlen = sequence_len)\n",
    "    y = np.array(vals['y'])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(glove_size):\n",
    "    glove = get_glove(glove_size)\n",
    "    glove_words = set(glove.keys())\n",
    "    default_vector = np.mean(list(glove.values()), axis = 0)\n",
    "    embedding_matrix = np.empty((len(word2ind), default_vector.shape[0]))\n",
    "    for word, position in word2ind.items():\n",
    "        embedding_matrix[position,:] = glove.get(word, default_vector)\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, embedding = 300, \n",
    "                 train_embeddings = True, \n",
    "                 pretrained_embedding = False, \n",
    "                 vocab_size = None,\n",
    "                 filter_windows = [3,4,5],\n",
    "                 feature_maps = 100,\n",
    "                 dropout_rate = .5,\n",
    "                 l2_constraint = 3):\n",
    "        super(CNNModel, self).__init__()\n",
    "        if pretrained_embedding:\n",
    "            embedding_matrix = get_embedding_matrix(embedding)\n",
    "            embedding_initializer = tf.keras.initializers.Constant(embedding_matrix)\n",
    "        else:\n",
    "            embedding_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "        if vocab_size is None:\n",
    "            vocab_size = len(word2ind)\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size, \n",
    "            embedding, \n",
    "            embeddings_initializer= embedding_initializer,\n",
    "            trainable = train_embeddings\n",
    "        )\n",
    "        self.filters = [tf.keras.layers.Conv1D(feature_maps, fw, activation = 'relu',kernel_constraint=tf.keras.constraints.max_norm(l2_constraint)) for fw in filter_windows]\n",
    "        self.max_pool = tf.keras.layers.GlobalMaxPool1D()\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.concat = tf.keras.layers.Concatenate(axis=1)\n",
    "        self.logistic = tf.keras.layers.Dense(1, activation = 'sigmoid', \n",
    "                                              kernel_constraint=tf.keras.constraints.max_norm(l2_constraint))\n",
    "        \n",
    "    def call(self, inputs, training = False):\n",
    "        x = self.embedding(inputs)\n",
    "        filter_outputs = []\n",
    "        for f in self.filters:\n",
    "            out = f(x)\n",
    "            filter_outputs.append(self.max_pool(out))\n",
    "        x = self.concat(filter_outputs)\n",
    "        if training:\n",
    "            x = self.dropout(x)\n",
    "        return self.logistic(x)\n",
    "            \n",
    "def compile_model(model = CNNModel, lr = .01, model_args = {}, metrics = ['BinaryAccuracy']):\n",
    "    net = model(**model_args)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "    net.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = metrics)\n",
    "    return net\n",
    "\n",
    "def run_model(model_type = CNNModel, verbose = 2, epochs = 100, batch_size = 50, \n",
    "              sequence_len = 45, learning_rate = .01, model_args = {}):\n",
    "    (xtrain, ytrain)= get_dataset('train', sequence_len)\n",
    "    (xval, yval) = get_dataset('val', sequence_len)\n",
    "    model = compile_model(model_type, learning_rate, model_args)\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                   patience=10, \n",
    "                                                   restore_best_weights = True)\n",
    "    train_history = model.fit(xtrain, ytrain, callbacks = [es_callback], validation_data = (xval, yval), batch_size = batch_size, epochs = epochs, verbose = verbose)\n",
    "    return train_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Run a basic CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rand_hist = run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6530 samples, validate on 845 samples\n",
      "Epoch 1/100\n",
      "6530/6530 - 7s - loss: 0.6413 - binary_accuracy: 0.6303 - val_loss: 0.5675 - val_binary_accuracy: 0.7290\n",
      "Epoch 2/100\n",
      "6530/6530 - 6s - loss: 0.5305 - binary_accuracy: 0.7686 - val_loss: 0.5194 - val_binary_accuracy: 0.7621\n",
      "Epoch 3/100\n",
      "6530/6530 - 6s - loss: 0.4699 - binary_accuracy: 0.8087 - val_loss: 0.4936 - val_binary_accuracy: 0.7669\n",
      "Epoch 4/100\n",
      "6530/6530 - 6s - loss: 0.4249 - binary_accuracy: 0.8372 - val_loss: 0.4786 - val_binary_accuracy: 0.7728\n",
      "Epoch 5/100\n",
      "6530/6530 - 6s - loss: 0.3897 - binary_accuracy: 0.8608 - val_loss: 0.4675 - val_binary_accuracy: 0.7822\n",
      "Epoch 6/100\n",
      "6530/6530 - 6s - loss: 0.3600 - binary_accuracy: 0.8798 - val_loss: 0.4647 - val_binary_accuracy: 0.7728\n",
      "Epoch 7/100\n",
      "6530/6530 - 6s - loss: 0.3330 - binary_accuracy: 0.8971 - val_loss: 0.4575 - val_binary_accuracy: 0.7811\n",
      "Epoch 8/100\n",
      "6530/6530 - 6s - loss: 0.3091 - binary_accuracy: 0.9133 - val_loss: 0.4529 - val_binary_accuracy: 0.7882\n",
      "Epoch 9/100\n",
      "6530/6530 - 6s - loss: 0.2854 - binary_accuracy: 0.9270 - val_loss: 0.4485 - val_binary_accuracy: 0.7941\n",
      "Epoch 10/100\n",
      "6530/6530 - 6s - loss: 0.2647 - binary_accuracy: 0.9406 - val_loss: 0.4504 - val_binary_accuracy: 0.7882\n",
      "Epoch 11/100\n",
      "6530/6530 - 6s - loss: 0.2447 - binary_accuracy: 0.9498 - val_loss: 0.4458 - val_binary_accuracy: 0.7917\n",
      "Epoch 12/100\n",
      "6530/6530 - 6s - loss: 0.2262 - binary_accuracy: 0.9564 - val_loss: 0.4503 - val_binary_accuracy: 0.7870\n",
      "Epoch 13/100\n",
      "6530/6530 - 6s - loss: 0.2091 - binary_accuracy: 0.9632 - val_loss: 0.4577 - val_binary_accuracy: 0.7882\n",
      "Epoch 14/100\n",
      "6530/6530 - 6s - loss: 0.1917 - binary_accuracy: 0.9730 - val_loss: 0.4459 - val_binary_accuracy: 0.7941\n",
      "Epoch 15/100\n",
      "6530/6530 - 6s - loss: 0.1767 - binary_accuracy: 0.9767 - val_loss: 0.4444 - val_binary_accuracy: 0.7953\n",
      "Epoch 16/100\n",
      "6530/6530 - 6s - loss: 0.1621 - binary_accuracy: 0.9821 - val_loss: 0.4485 - val_binary_accuracy: 0.7941\n",
      "Epoch 17/100\n",
      "6530/6530 - 6s - loss: 0.1486 - binary_accuracy: 0.9855 - val_loss: 0.4449 - val_binary_accuracy: 0.8036\n",
      "Epoch 18/100\n",
      "6530/6530 - 6s - loss: 0.1359 - binary_accuracy: 0.9896 - val_loss: 0.4447 - val_binary_accuracy: 0.7917\n",
      "Epoch 19/100\n",
      "6530/6530 - 6s - loss: 0.1242 - binary_accuracy: 0.9930 - val_loss: 0.4470 - val_binary_accuracy: 0.7964\n",
      "Epoch 20/100\n",
      "6530/6530 - 6s - loss: 0.1136 - binary_accuracy: 0.9949 - val_loss: 0.4484 - val_binary_accuracy: 0.7917\n",
      "Epoch 21/100\n",
      "6530/6530 - 6s - loss: 0.1041 - binary_accuracy: 0.9965 - val_loss: 0.4521 - val_binary_accuracy: 0.7917\n",
      "Epoch 22/100\n",
      "6530/6530 - 6s - loss: 0.0951 - binary_accuracy: 0.9979 - val_loss: 0.4501 - val_binary_accuracy: 0.7988\n",
      "Epoch 23/100\n",
      "6530/6530 - 6s - loss: 0.0872 - binary_accuracy: 0.9975 - val_loss: 0.4572 - val_binary_accuracy: 0.7929\n",
      "Epoch 24/100\n",
      "6530/6530 - 6s - loss: 0.0791 - binary_accuracy: 0.9986 - val_loss: 0.4526 - val_binary_accuracy: 0.8000\n",
      "Epoch 25/100\n",
      "6530/6530 - 6s - loss: 0.0725 - binary_accuracy: 0.9985 - val_loss: 0.4584 - val_binary_accuracy: 0.7929\n"
     ]
    }
   ],
   "source": [
    "static_hist = run_model(model_args = {'pretrained_embedding': True, 'train_embeddings': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6530 samples, validate on 845 samples\n",
      "Epoch 1/100\n",
      "6530/6530 - 15s - loss: 0.6394 - binary_accuracy: 0.6297 - val_loss: 0.5594 - val_binary_accuracy: 0.7515\n",
      "Epoch 2/100\n",
      "6530/6530 - 14s - loss: 0.5208 - binary_accuracy: 0.7821 - val_loss: 0.5128 - val_binary_accuracy: 0.7751\n",
      "Epoch 3/100\n",
      "6530/6530 - 14s - loss: 0.4508 - binary_accuracy: 0.8250 - val_loss: 0.4911 - val_binary_accuracy: 0.7692\n",
      "Epoch 4/100\n",
      "6530/6530 - 14s - loss: 0.3968 - binary_accuracy: 0.8591 - val_loss: 0.4733 - val_binary_accuracy: 0.7799\n",
      "Epoch 5/100\n",
      "6530/6530 - 14s - loss: 0.3518 - binary_accuracy: 0.8856 - val_loss: 0.4573 - val_binary_accuracy: 0.7882\n",
      "Epoch 6/100\n",
      "6530/6530 - 15s - loss: 0.3117 - binary_accuracy: 0.9107 - val_loss: 0.4484 - val_binary_accuracy: 0.7882\n",
      "Epoch 7/100\n",
      "6530/6530 - 14s - loss: 0.2769 - binary_accuracy: 0.9296 - val_loss: 0.4423 - val_binary_accuracy: 0.7941\n",
      "Epoch 8/100\n",
      "6530/6530 - 14s - loss: 0.2451 - binary_accuracy: 0.9436 - val_loss: 0.4388 - val_binary_accuracy: 0.7882\n",
      "Epoch 9/100\n",
      "6530/6530 - 14s - loss: 0.2162 - binary_accuracy: 0.9582 - val_loss: 0.4349 - val_binary_accuracy: 0.7953\n",
      "Epoch 10/100\n",
      "6530/6530 - 14s - loss: 0.1898 - binary_accuracy: 0.9703 - val_loss: 0.4404 - val_binary_accuracy: 0.7941\n",
      "Epoch 11/100\n",
      "6530/6530 - 14s - loss: 0.1659 - binary_accuracy: 0.9761 - val_loss: 0.4326 - val_binary_accuracy: 0.7929\n",
      "Epoch 12/100\n",
      "6530/6530 - 14s - loss: 0.1452 - binary_accuracy: 0.9809 - val_loss: 0.4326 - val_binary_accuracy: 0.8000\n",
      "Epoch 13/100\n",
      "6530/6530 - 14s - loss: 0.1264 - binary_accuracy: 0.9864 - val_loss: 0.4320 - val_binary_accuracy: 0.7976\n",
      "Epoch 14/100\n",
      "6530/6530 - 15s - loss: 0.1099 - binary_accuracy: 0.9908 - val_loss: 0.4343 - val_binary_accuracy: 0.8012\n",
      "Epoch 15/100\n",
      "6530/6530 - 15s - loss: 0.0954 - binary_accuracy: 0.9943 - val_loss: 0.4356 - val_binary_accuracy: 0.8012\n",
      "Epoch 16/100\n",
      "6530/6530 - 14s - loss: 0.0832 - binary_accuracy: 0.9962 - val_loss: 0.4383 - val_binary_accuracy: 0.8012\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-9f1f9d5cc1fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnonstatic_hist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstatic_hist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'pretrained_embedding'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train_embeddings'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-89-ecc8d7de0719>\u001b[0m in \u001b[0;36mrun_model\u001b[1;34m(model_type, verbose, epochs, batch_size, sequence_len, learning_rate, model_args)\u001b[0m\n\u001b[0;32m     55\u001b[0m                                                    \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                                                    restore_best_weights = True)\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mtrain_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mes_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mxval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nonstatic_hist = static_hist = run_model(model_args = {'pretrained_embedding': True, 'train_embeddings': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.5134511198517839,\n",
       "  'val_binary_accuracy': 0.76094675,\n",
       "  'itter': 13},\n",
       " {'val_loss': 0.458090587833224,\n",
       "  'val_binary_accuracy': 0.78934914,\n",
       "  'itter': 14},\n",
       " {'val_loss': 0.4694531284845792,\n",
       "  'val_binary_accuracy': 0.7727811,\n",
       "  'itter': 11}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_history(histories, title = None, sub_titles = None, width = 2, plot_metrics = None):\n",
    "    if plot_metrics is not None:\n",
    "        plot_metrics += ['val_' + m for m in plot_metrics]\n",
    "    else:\n",
    "        plot_metrics = list(histories[0].history.keys())\n",
    "    histories = [h.history for h in histories]\n",
    "    n_plots = len(histories)\n",
    "    options = {'figsize': [20,30]}\n",
    "    fig, ax = plt.subplots(n_plots, **options)\n",
    "    if title is not None:\n",
    "        fig.suptitle(title)\n",
    "    for i in range(n_plots):\n",
    "        history = histories[i]\n",
    "        for key in plot_metrics:\n",
    "            vals = history[key]\n",
    "            ax[i].plot(range(len(vals)), vals, label=key)\n",
    "            if sub_titles is not None:\n",
    "                ax[i].set_title(sub_titles[i])\n",
    "        if len(history.keys()) > 1:\n",
    "            ax[i].legend()\n",
    "plot_history(vanilla_hists,\n",
    "             title = 'Training and Validation loss for 1 layer RNN variants', \n",
    "             sub_titles = ['Rnn','LSTM','GRU'])\n",
    "#report the number of itterations that got the best validation accuracy\n",
    "def best_timestep(history, metric_name = 'val_binary_accuracy', big_good = True):\n",
    "    values = history.history[metric_name]\n",
    "    evals = [np.mean( values[np.max([i-1,0]): np.min([i+1,len(values)-1])] ) for i in range(len(values))]\n",
    "    if not big_good:\n",
    "        evals = [-i for i in evals]\n",
    "    best_itter = np.argmax(evals)\n",
    "    best_val = values[best_itter]\n",
    "    result = {key: value[best_itter] for key, value in history.history.items() if re.search('val_', key) is not None}\n",
    "    result['itter'] = best_itter\n",
    "    return result\n",
    "[best_timestep(hist) for hist in vanilla_hists]\n",
    "\n",
    "def get_best_results(history_list, configs):\n",
    "    best_result_list = [best_timestep(hist) for hist in history_list]\n",
    "    best_config = configs[ np.argmax([n['val_binary_accuracy'] for n in best_result_list]) ]\n",
    "    print([n['val_binary_accuracy'] for n in best_result_list])\n",
    "    return best_config\n",
    "\n",
    "def lineplot_small_multiples(histories, title = None, sub_title_func = None, width = 2):\n",
    "    histories = [h.history for h in histories]\n",
    "    n_plots = len(histories)\n",
    "    n_rows = int(np.ceil(n_plots/width))\n",
    "    options = {'figsize': [40/n_rows,30/width]}\n",
    "    fig, ax = plt.subplots(n_rows, width, **options)\n",
    "    if title is not None:\n",
    "        fig.suptitle(title)\n",
    "    for i in range(n_rows*width):\n",
    "        x = int(i%width)\n",
    "        y = int(np.floor(i/width))\n",
    "        if n_rows > 1 and width > 1:\n",
    "            axis = ax[y,x]\n",
    "        elif width > 1:\n",
    "            axis = ax[x]\n",
    "        else:\n",
    "            axis = ax[y]\n",
    "        if i < n_plots:\n",
    "            history = histories[i]\n",
    "            for key, vals in history.items():\n",
    "                axis.plot(range(len(vals)), vals, label=key)\n",
    "                if sub_title_func is not None:\n",
    "                    axis.set_title(sub_title_func(i))\n",
    "            if len(history.keys()) > 1:\n",
    "                axis.legend()\n",
    "        else:\n",
    "            fig.delaxes(axis)\n",
    "            \n",
    "def find_best_model(model_type, seq_len = 50, epochs = 40):\n",
    "    xtrain, ytrain = get_dataset('train', seq_len)\n",
    "    xval, yval = get_dataset('val', seq_len)\n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                patience=2, \n",
    "                                restore_best_weights = True)\n",
    "    for embedding in [50,100,300]:\n",
    "        for hidden_states in [[4], [8], [16]]:    \n",
    "            for train_embeddings in [True, False]:\n",
    "                model_args = {'hidden_states': hidden_states,\n",
    "                             'embedding': embedding,\n",
    "                             'train_embeddings': train_embeddings}\n",
    "                model = compile_model(model_type, .005, model_args, \n",
    "                                      ['BinaryAccuracy','Precision','Recall'])\n",
    "                hist = model.fit(xtrain, ytrain,\n",
    "                          validation_data = (xval, yval),\n",
    "                          callbacks = [es_callback],\n",
    "                          batch_size = 500,\n",
    "                          epochs = epochs,\n",
    "                          verbose = 0)\n",
    "                score = np.max(hist.history['val_binary_accuracy'])\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_model = model\n",
    "                    print(score)\n",
    "    return model\n",
    "\n",
    "def evaluate_best_model(model_type):\n",
    "    model = find_best_model(model_type)\n",
    "    xtest, ytest = get_dataset('test', 50)\n",
    "    [eval_loss, eval_acc, eval_precision, eval_recall] = model.evaluate(xtest, ytest)\n",
    "    f1_score = 2*eval_precision*eval_recall/(eval_precision + eval_recall)\n",
    "    result = {'precision': eval_precision, 'recall': eval_recall, 'f1_score': f1_score}\n",
    "    return model.__class__.__name__, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
